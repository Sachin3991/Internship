{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da66b0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bs4\n",
      "  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in d:\\python\\lib\\site-packages (from bs4) (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in d:\\python\\lib\\site-packages (from beautifulsoup4->bs4) (2.3.1)\n",
      "Building wheels for collected packages: bs4\n",
      "  Building wheel for bs4 (setup.py): started\n",
      "  Building wheel for bs4 (setup.py): finished with status 'done'\n",
      "  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1272 sha256=d7cf6991c63c25f76d0c02f13348695a197f53cd24115950bea193d9705b5815\n",
      "  Stored in directory: c:\\users\\91797\\appdata\\local\\pip\\cache\\wheels\\73\\2b\\cb\\099980278a0c9a3e57ff1a89875ec07bfa0b6fcbebb9a8cad3\n",
      "Successfully built bs4\n",
      "Installing collected packages: bs4\n",
      "Successfully installed bs4-0.0.1\n",
      "Requirement already satisfied: requests in d:\\python\\lib\\site-packages (2.27.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\python\\lib\\site-packages (from requests) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\python\\lib\\site-packages (from requests) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\python\\lib\\site-packages (from requests) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in d:\\python\\lib\\site-packages (from requests) (2.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4\n",
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ec02f5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests,re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0af0f054",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 1 (program to display all header tags from wikipedia)\n",
    "def wiki(str):\n",
    "    page=requests.get(str)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    headers=['h1','h2','h3','h4','h5','h6']\n",
    "    for i in soup.find_all(headers):\n",
    "        print(i.name,\" \",i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7eb106b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the URL of the website https://en.wikipedia.org/wiki/Main_Page\n",
      "h1   Main Page\n",
      "h1   Welcome to Wikipedia\n",
      "h2   From today's featured article\n",
      "h2   Did you know ...\n",
      "h2   In the news\n",
      "h2   On this day\n",
      "h2   From today's featured list\n",
      "h2   Today's featured picture\n",
      "h2   Other areas of Wikipedia\n",
      "h2   Wikipedia's sister projects\n",
      "h2   Wikipedia languages\n",
      "h2   Navigation menu\n",
      "h3   \n",
      "Personal tools\n",
      "\n",
      "h3   \n",
      "Namespaces\n",
      "\n",
      "h3   \n",
      "Views\n",
      "\n",
      "h3   \n",
      "Search\n",
      "\n",
      "h3   \n",
      "Navigation\n",
      "\n",
      "h3   \n",
      "Contribute\n",
      "\n",
      "h3   \n",
      "Tools\n",
      "\n",
      "h3   \n",
      "Print/export\n",
      "\n",
      "h3   \n",
      "In other projects\n",
      "\n",
      "h3   \n",
      "Languages\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s=str(input('Enter the URL of the website '))\n",
    "wiki(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dd7ce99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 2 (python program to display IMDB’s Top rated 100 movies’ data)\n",
    "def imdb_data(str):\n",
    "    page=requests.get(str)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    movie_name=[]\n",
    "    rating=[]\n",
    "    movie_year=[]\n",
    "    for i in soup.find_all('h3',class_='lister-item-header'):\n",
    "        movie_name.append(i.text.split(\"\\n\")[2])\n",
    "    for j in soup.find_all('span', class_='lister-item-year text-muted unbold'):\n",
    "        movie_year.append(j.text)\n",
    "    for k in soup.find_all('div', class_='inline-block ratings-imdb-rating'):\n",
    "        rating.append(k.text.split(\"\\n\")[2])\n",
    "    data={\n",
    "        'name':movie_name, 'year':movie_year,'rating':rating\n",
    "    }\n",
    "    df=pd.DataFrame(data)\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d8668ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            name    year rating\n",
      "0                       The Shawshank Redemption  (1994)    9.3\n",
      "1                                  The Godfather  (1972)    9.2\n",
      "2                                The Dark Knight  (2008)    9.0\n",
      "3  The Lord of the Rings: The Return of the King  (2003)    9.0\n",
      "4                               Schindler's List  (1993)    9.0\n"
     ]
    }
   ],
   "source": [
    "s='https://www.imdb.com/search/title/?groups=top_100&sort=user_rating,desc&ref_=adv_prv'\n",
    "imdb_data(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee1ef569",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 3 (Python program to display IMDB's top rated 100 Indian movies)\n",
    "def imdb_hindi(str):\n",
    "    page=requests.get(str)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    movie_name=[]\n",
    "    movie_rating=[]\n",
    "    movie_year=[]\n",
    "    for i in soup.find_all('h3',class_='lister-item-header'):\n",
    "        movie_name.append(i.text.split(\"\\n\")[2])\n",
    "    for j in soup.find_all('span',class_='lister-item-year text-muted unbold'):\n",
    "        movie_year.append(j.text)\n",
    "    for k in soup.find_all('div',class_='ipl-rating-star small'):\n",
    "        movie_rating.append(k.text.split(\"\\n\")[8])\n",
    "    hindi_movies={\n",
    "        'name':movie_name,'rating':movie_rating,'year of release':movie_year\n",
    "    }\n",
    "    hindi_movies_list=pd.DataFrame(hindi_movies)\n",
    "    print(hindi_movies_list.head())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e87d37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     name rating year of release\n",
      "0         Rang De Basanti    8.1          (2006)\n",
      "1                3 Idiots    8.4          (2009)\n",
      "2        Taare Zameen Par    8.4          (2007)\n",
      "3          Dil Chahta Hai    8.1          (2001)\n",
      "4  Swades: We, the People    8.2          (2004)\n"
     ]
    }
   ],
   "source": [
    "s='https://www.imdb.com/list/ls009997493/'\n",
    "imdb_hindi(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "23c9b426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 4 (python program to display list of respected former presidents of India)\n",
    "def president(str):\n",
    "    page=requests.get(str)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    name=[]\n",
    "    term=[]\n",
    "    for i in soup.find_all('div',class_='presidentListing'):\n",
    "        s=i.text.split(\"\\n\")[1]\n",
    "        s1=s.split(\"(\")[0]\n",
    "        name.append(s1)\n",
    "    for j in soup.find_all('div',class_='presidentListing'):\n",
    "        s2=j.text.split(\":\")[1]\n",
    "        s3=s2.split(\"\\n\")[0]\n",
    "        term.append(s3)\n",
    "    prez={\n",
    "        'Name':name,'Term of Office':term\n",
    "    }\n",
    "    president_data=pd.DataFrame(prez)\n",
    "    print(president_data.head())\n",
    "        \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1f03de8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            Name                    Term of Office\n",
      "0          Shri Ram Nath Kovind    25 July, 2017 to 25 July, 2022 \n",
      "1         Shri Pranab Mukherjee    25 July, 2012 to 25 July, 2017 \n",
      "2  Smt Pratibha Devisingh Patil    25 July, 2007 to 25 July, 2012 \n",
      "3        DR. A.P.J. Abdul Kalam    25 July, 2002 to 25 July, 2007 \n",
      "4          Shri K. R. Narayanan    25 July, 1997 to 25 July, 2002 \n"
     ]
    }
   ],
   "source": [
    "s='https://presidentofindia.nic.in/former-presidents.htm'\n",
    "president(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "28bb1e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 5 (Python program to scrape cricket rankings)\n",
    "def cricket_teams(str):\n",
    "    page=requests.get(str)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    team_name=[]\n",
    "    team_matches=[]\n",
    "    team_points=[]\n",
    "    team_rating=[]\n",
    "    for i in soup.find_all('span',class_='u-hide-phablet'):\n",
    "        team_name.append(i.text)\n",
    "    first_element=soup.find('td',class_='rankings-block__banner--matches')\n",
    "    team_matches.append(first_element)\n",
    "    for j in soup.find_all('tr',class_='table-body'):\n",
    "        team_matches.append(j.text)\n",
    "    first_element=soup.find('td',class_='rankings-block__banner--points')\n",
    "    team_points.append(first_element)\n",
    "    for k in soup.find_all('tr',class_='table-body'):\n",
    "        team_points.append(k.text)\n",
    "    first_element=soup.find('td',class_='rankings-block__banner--rating u-text-right')\n",
    "    team_rating.append(first_element)\n",
    "    for p in soup.find_all('tr',class_='table-body'):\n",
    "        team_rating.append(p.text)\n",
    "    cric_odi={\n",
    "        'Name':team_name,'Matches':team_matches,'Points':team_points,'Rating':team_rating\n",
    "    }\n",
    "    crick_odi=pd.DataFrame(cric_odi)\n",
    "    print(crick_odi.head())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "34e7a287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Name                                       Matches  \\\n",
      "0      England                                          [27]   \n",
      "1  New Zealand  \\n2\\n\\n\\nNew Zealand\\nNZ\\n\\n22\\n2,508\\n114\\n   \n",
      "2        India       \\n3\\n\\n\\nIndia\\nIND\\n\\n34\\n3,802\\n112\\n   \n",
      "3     Pakistan    \\n4\\n\\n\\nPakistan\\nPAK\\n\\n22\\n2,354\\n107\\n   \n",
      "4    Australia   \\n5\\n\\n\\nAustralia\\nAUS\\n\\n29\\n3,071\\n106\\n   \n",
      "\n",
      "                                         Points  \\\n",
      "0                                       [3,226]   \n",
      "1  \\n2\\n\\n\\nNew Zealand\\nNZ\\n\\n22\\n2,508\\n114\\n   \n",
      "2       \\n3\\n\\n\\nIndia\\nIND\\n\\n34\\n3,802\\n112\\n   \n",
      "3    \\n4\\n\\n\\nPakistan\\nPAK\\n\\n22\\n2,354\\n107\\n   \n",
      "4   \\n5\\n\\n\\nAustralia\\nAUS\\n\\n29\\n3,071\\n106\\n   \n",
      "\n",
      "                                              Rating  \n",
      "0  [\\n                            119\\n          ...  \n",
      "1       \\n2\\n\\n\\nNew Zealand\\nNZ\\n\\n22\\n2,508\\n114\\n  \n",
      "2            \\n3\\n\\n\\nIndia\\nIND\\n\\n34\\n3,802\\n112\\n  \n",
      "3         \\n4\\n\\n\\nPakistan\\nPAK\\n\\n22\\n2,354\\n107\\n  \n",
      "4        \\n5\\n\\n\\nAustralia\\nAUS\\n\\n29\\n3,071\\n106\\n  \n"
     ]
    }
   ],
   "source": [
    "s='https://www.icc-cricket.com/rankings/mens/team-rankings/odi'\n",
    "cricket_teams(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e6b7b279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting html5lib\n",
      "  Downloading html5lib-1.1-py2.py3-none-any.whl (112 kB)\n",
      "Requirement already satisfied: six>=1.9 in d:\\python\\lib\\site-packages (from html5lib) (1.16.0)\n",
      "Requirement already satisfied: webencodings in d:\\python\\lib\\site-packages (from html5lib) (0.5.1)\n",
      "Installing collected packages: html5lib\n",
      "Successfully installed html5lib-1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install html5lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f8be4ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 7 (python program to scrape mentioned news details from https://www.cnbc.com/world/?region=world)\n",
    "def news(str):\n",
    "    page=requests.get(str)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    headline=[]\n",
    "    time=[]\n",
    "    link=[]\n",
    "    for i in soup.find_all('a',class_='LatestNews-headline'):\n",
    "        headline.append(i.text)\n",
    "    for j in soup.find_all('time',class_='LatestNews-timestamp'):\n",
    "        time.append(j.text)\n",
    "    for k in soup.find_all('a',class_='LatestNews-headline'):\n",
    "        if(k.get('href')!='#'):\n",
    "            link.append('https://www.cnbc.com/'+k.get('href'))\n",
    "        \n",
    "    cnbcnews={\n",
    "        'headline':headline,'time':time,'link':link\n",
    "    }\n",
    "    cnbc=pd.DataFrame(cnbcnews)\n",
    "    print(cnbc.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "de0a4b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            headline         time  \\\n",
      "0  Members of Congress express support for Paul P...   1 Hour Ago   \n",
      "1  Haunted houses, scary movies may actually redu...  2 Hours Ago   \n",
      "2  Study: Women don't really need to lower their ...  3 Hours Ago   \n",
      "3  Diesel market is in perfect storm as prices su...  3 Hours Ago   \n",
      "4  Bitcoin's break above $20,000 has investors wo...  3 Hours Ago   \n",
      "\n",
      "                                                link  \n",
      "0  https://www.cnbc.com/https://www.cnbc.com/2022...  \n",
      "1  https://www.cnbc.com/https://www.cnbc.com/2022...  \n",
      "2  https://www.cnbc.com/https://www.cnbc.com/2022...  \n",
      "3  https://www.cnbc.com/https://www.cnbc.com/2022...  \n",
      "4  https://www.cnbc.com/https://www.cnbc.com/2022...  \n"
     ]
    }
   ],
   "source": [
    "s='https://www.cnbc.com/world/?region=world'\n",
    "news(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "21dfc7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 8 (python program to scrape the details of most downloaded articles from AI in last 90 days)\n",
    "def article(str):\n",
    "    page=requests.get(str)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    paper_title=[]\n",
    "    authors=[]\n",
    "    published_date=[]\n",
    "    paperurl=[]\n",
    "    for i in soup.find_all('h2',class_='sc-1qrq3sd-1 MKjKb sc-1nmom32-0 sc-1nmom32-1 hqhUYH ebTA-dR'):\n",
    "        paper_title.append(i.text)\n",
    "    for j in soup.find_all('span',class_='sc-1w3fpd7-0 pgLAT'):\n",
    "        authors.append(j.text)\n",
    "    for k in soup.find_all('span',class_='sc-1thf9ly-2 bKddwo'):\n",
    "        published_date.append(k.text)\n",
    "    for p in soup.find_all('a',class_='sc-5smygv-0 nrDZj'):\n",
    "        if(p.get('href')!='#'):\n",
    "            paperurl.append(p.get('href'))\n",
    "    articles={\n",
    "    'paper title':paper_title,'authors':authors,'published date':published_date,'paper url':paperurl\n",
    "    }\n",
    "    AIarticles=pd.DataFrame(articles)\n",
    "    print(AIarticles.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a1b34327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         paper title  \\\n",
      "0                                   Reward is enough   \n",
      "1                          Making sense of raw input   \n",
      "2  Law and logic: A review from an argumentation ...   \n",
      "3             Creativity and artificial intelligence   \n",
      "4  Artificial cognition for social human–robot in...   \n",
      "\n",
      "                                             authors published date  \\\n",
      "0  Silver, David, Singh, Satinder, Precup, Doina,...   October 2021   \n",
      "1          Evans, Richard, Bošnjak, Matko and 5 more   October 2021   \n",
      "2                  Prakken, Henry, Sartor, Giovanni    October 2015   \n",
      "3                                Boden, Margaret A.     August 1998   \n",
      "4    Lemaignan, Séverin, Warnier, Mathieu and 3 more      June 2017   \n",
      "\n",
      "                                           paper url  \n",
      "0  https://www.sciencedirect.com/science/article/...  \n",
      "1  https://www.sciencedirect.com/science/article/...  \n",
      "2  https://www.sciencedirect.com/science/article/...  \n",
      "3  https://www.sciencedirect.com/science/article/...  \n",
      "4  https://www.sciencedirect.com/science/article/...  \n"
     ]
    }
   ],
   "source": [
    "s='https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles'\n",
    "article(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "12c77a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 9 (python program to scrape mentioned details from dineout.co.in )\n",
    "def hotel(str):\n",
    "    page=requests.get(str)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    resto_name=[]\n",
    "    cuisine=[]\n",
    "    location=[]\n",
    "    ratings=[]\n",
    "    imageurl=[]\n",
    "    for i in soup.find_all('a',class_='restnt-name ellipsis'):\n",
    "        resto_name.append(i.text)\n",
    "    for j in soup.find_all('span',class_='double-line-ellipsis'):\n",
    "        cuisine.append(j.text.split(\"|\")[1])\n",
    "    for k in soup.find_all('div',class_='restnt-loc ellipsis'):\n",
    "        location.append(k.text)\n",
    "    for p in soup.find_all('div',class_='img-wrap'):\n",
    "        ratings.append(p.text)\n",
    "    for m in soup.find_all('img',class_='no-img'):\n",
    "        imageurl.append(m.get('data-src'))\n",
    "            \n",
    "    hotel_data={\n",
    "        'name':resto_name,'cuisine':cuisine,'location':location,'ratings':ratings,'imageurl':imageurl\n",
    "    }\n",
    "    hotels=pd.DataFrame(hotel_data)\n",
    "    print(hotels.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "27835779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   name                        cuisine  \\\n",
      "0       Castle Barbeque          Chinese, North Indian   \n",
      "1       Jungle Jamboree   North Indian, Asian, Italian   \n",
      "2       Castle Barbeque          Chinese, North Indian   \n",
      "3            Cafe Knosh           Italian, Continental   \n",
      "4  The Barbeque Company          North Indian, Chinese   \n",
      "\n",
      "                                            location ratings  \\\n",
      "0                     Connaught Place, Central Delhi     4.1   \n",
      "1             3CS Mall,Lajpat Nagar - 3, South Delhi     3.9   \n",
      "2             Pacific Mall,Tagore Garden, West Delhi     3.9   \n",
      "3  The Leela Ambience Convention Hotel,Shahdara, ...     4.3   \n",
      "4                 Gardens Galleria,Sector 38A, Noida       4   \n",
      "\n",
      "                                            imageurl  \n",
      "0  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "1  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "2  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "3  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "4  https://im1.dineout.co.in/images/uploads/resta...  \n"
     ]
    }
   ],
   "source": [
    "s='https://www.dineout.co.in/delhi-restaurants/buffet-special'\n",
    "hotel(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d6f01732",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 10 (Write a python program to scrape the details of top publications from Google Scholar )\n",
    "def publications(str):\n",
    "    page=requests.get(str)\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    rank=[]\n",
    "    publication=[]\n",
    "    h5index=[]\n",
    "    h5median=[]\n",
    "    for i in soup.find_all('tr'):\n",
    "        rank.append(i.text.split(\".\")[0])\n",
    "    for j in soup.find_all('tr'):\n",
    "        s1=j.text[2:len(j.text)-6]\n",
    "        publication.append(s1)\n",
    "    for k in soup.find_all('tr'):\n",
    "        s=k.text[len(k.text)-6:len(k.text)-3]\n",
    "        h5index.append(s)\n",
    "    for p in soup.find_all('tr'):\n",
    "        s2=p.text[len(p.text)-3:len(p.text)]\n",
    "        h5median.append(s2)\n",
    "    publish={\n",
    "        'rank':rank,'publication':publication,'h5 index':h5index,'h5 median':h5median\n",
    "    }\n",
    "    published=pd.DataFrame(publish)\n",
    "    published=published.drop(0)\n",
    "    print(published.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "1a4a81dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  rank                                        publication h5 index h5 median\n",
      "1    1                                             Nature      444       667\n",
      "2    2                The New England Journal of Medicine      432       780\n",
      "3    3                                            Science      401       614\n",
      "4    4  IEEE/CVF Conference on Computer Vision and Pat...      389       627\n",
      "5    5                                         The Lancet      354       635\n"
     ]
    }
   ],
   "source": [
    "s='https://scholar.google.com/citations?view_op=top_venues&hl=en'\n",
    "publications(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e93da01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfadb97e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138969f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
